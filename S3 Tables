# S3 Tables

GA: 03.12.2024
Purpose-build to store tabular data and provide fully-managed Apache Iceberg tables in S3.
Table bucket: Store Apache Iceberg format tables natively in S3 and S3 now supports tables as a new construct.

**S3 Tables:**
1. Each of the table has it's own ARN,
2. Resource policy can be written directly against them.
3. Has dedicated endpoint s3tables.region.amazonaws.com that can be used to access these resources.

**Tables Operations**
1. S3tables:ListTable
2. S3tables:CreateTable
3. S3tables:GetTableMetadataLocation
4. S3tables:UpdateTableMetadataLocation
5. S3tables:DeleteTable
CRUP operations on the tables directly.

**Table Management**
1. S3tables:PutTablePolicy
2. S3tables:PutTableBucketPolicy
3. S3tables:PutTableMaintenanceConfig
4. S3tables:PutTableBucketMaintenanceConfig
They are both at bucket level and s3table bucket level.
How long you want your snapshot to be available?
How long you want to take to age them out?

**New Protection**
1. No delete allowed of existing objects
2. No overwrite allowed of existing objects
3. Audit with AWS CloudTrail
2 & 3 are in place so that the integrity of the table is maintained.
Right way to delete/overwrite the object is throught the maintenance policies.

Additional benefits:
1. Optimised Performance
2. Security Controls
3. Cost Optimisation (automatic)

**Optimised Performance:S3 Tables""
1. 10x Transactions Per Second (TPS)
2. 3x Faster Query Performance
S3tables are purpose-build of tables, so as soon S3recognised theis dataset is in tabular format that you are storing in iceberg format. AWS specifically apply optimisation on our behalf 

**Performance benchmarks**
General purpose buckets: 
Scale by prefix
Adds TPS capacity under load, 
starts at 5500 reads/seconds
starts at 3500 writes/seconds
Under load S3 request additional resources to achieve the demand.

Table buckets
S3 storage optimised for iceberg key naming
TPS startes at 55000 read/seconds
35000 write/seconds
AWS contributed to Iceberg: in which you are writing your data and laying out your data files in Iceberg now is better optimized to be able to take advantage
of the automatic scaling that S3 provides. Blog post is also available [2]

Overtimes tables will continue to grow and this means query will be more because it has to go through now additional files.
Iceberg provides a way to solve this i.e. do the compaction of the smaller data files into the larget ones.
This traditionally need developers to do this. Now in S3 this can be achieved in the background and automatically leads to the shorter query times.
Can lead uptom 3x improvement in the query times. Blog post is linked[3]

**Security Controls**
Now that S3Tables itself are resources you can apply resource polidy to them and manage read/write permissions to them directly.
(Allowing you to be able to govern access to the table and the table bucket itself.)

Namespace: Logically group tables within table buckets.

S3 Tables - Automated maintenance: 
Any update in an iceberg tables result in a commit operation and that create a brand new snapshot for that table.
And this is great property of icebreg that lets you go back in time and be able to see a view of the table from the past and be able to rollback to it.
Old snapshots need not to stay forever as you will be pointing your dataset to the newest snapshot.
To manage this applications needs teams to do that. Now AWS can take care of that for us.
The S3 tables will make sure old snapshort are agin out(deleted?) and the associated files and garbage collector(clean up unreferenced file associated with older snapshot) for that files are run. Elimiinating the manual effort.
All of these procedures are policy driven. So that you can define how many of these snapshots you want to keep around and for how long you want to keep them around.
Also checking the integrity of the table i.e. you can go back and look up ho maintenance operations are faring for those tables.


**S3**
Originally build: 2006
Idea: Store the data securely & durably have it available when needed.
Envisioned filed stored: Video, Photos and other static content
Current usage: Data used for Analytics, Machine Learning and AI 
Usage: Meterological data, financial transactions and records
Format: Apache Parquet (Tabular format)

Problem with Parquet datafiles: Evolving the schema was difficult because adding and removing columns means that efforts is spent in making sure data can be consumed at speeed at scale. Does this mean we need to update schema in each and every application?
Iceberg: Adds metadata file, metadata allows to structure the data, safe transactions, time-travel, SQL semantic, allows consistency of the data

Iceberg features
1. Time travel
2. Schema information
3. Compaction


**S3 Tables in action**
James: Pr. Engineer at S3 team

Table on it own do not give us the possibility to read the data stored in the table i.e. read the bit or byts.
AWS Analytics:
1. Amazon EMR
2. Amazon Athena
3. Amazon Redshift
4. Amazon Quicksight
5. Amazon Data Firehose
Centre of all this integration is Glue Data Catalogue. Long time Glue Data Catalogue has supoprted tables stored in self managed S3 buckets.
That means we have to managed the tables ourselves 

+ With S3 Tabels your tables are automatically registered with the Glue Data Catalogue so that we do not have to make sure that every table is registered there for the access.

+ Ones the tables are registered with the Glue Data Catalogue they aitomatically show-up across the AWS Analytics ecosystem.
i.e. query the data using the Athena, perform deep analysis using Redshift and stream the data directly into your S3 tables using Amazon Data Firehose
Constrcut visualization using Amazon quicksight.
You can connect data sources like Kinesis and CloudWatch log and IoT devices.

S3 tables is a native iceberg product. There is a rich ecosystem both inside the AWS ecosystem that is known to write the data into the Iceberg tables but also third-party services that know how to generate the Iceberg tables and store them somewhere you would like them to.

Analyse the data that is stores in teh S3 Tables
Option 1: Amazon Athena: Serverless sequel engine that can natively query S3 Tables through Glue Table Catalogue.
```Select * from catalogu:name.namespace_of_tables.table_name```


Option 2: Amazon Redshift

## Reference: 
1. https://www.youtube.com/watch?v=1U7yX4HTLCI
2. https://aws.amazon.com/blogs/storage/how-amazon-ads-uses-iceberg-optimizations-to-accelerate-their-spark-workload-on-amazon-s3/
3. https://aws.amazon.com/blogs/storage/how-amazon-s3-tables-use-compaction-to-improve-query-performance-by-up-to-3-times/

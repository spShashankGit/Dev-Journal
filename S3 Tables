# S3 Tables

GA: 03.12.2024
Purpose-build to store tabular data and provide fully-managed Apache Iceberg tables in S3.
Table bucket: Store Apache Iceberg format tables natively in S3 and S3 now supports tables as a new construct.

**S3 Tables:**
1. Each of the table has it's own ARN,
2. Resource policy can be written directly against them.
3. Has dedicated endpoint s3tables.region.amazonaws.com that can be used to access these resources.

**Tables Operations**
1. S3tables:ListTable
2. S3tables:CreateTable
3. S3tables:GetTableMetadataLocation
4. S3tables:UpdateTableMetadataLocation
5. S3tables:DeleteTable
CRUP operations on the tables directly.

**Table Management**
1. S3tables:PutTablePolicy
2. S3tables:PutTableBucketPolicy
3. S3tables:PutTableMaintenanceConfig
4. S3tables:PutTableBucketMaintenanceConfig
They are both at bucket level and s3table bucket level.
How long you want your snapshot to be available?
How long you want to take to age them out?

**New Protection**
1. No delete allowed of existing objects
2. No overwrite allowed of existing objects
3. Audit with AWS CloudTrail
2 & 3 are in place so that the integrity of the table is maintained.
Right way to delete/overwrite the object is throught the maintenance policies.

Additional benefits:
1. Optimised Performance
2. Security Controls
3. Cost Optimisation (automatic)

**Optimised Performance:S3 Tables""
1. 10x Transactions Per Second (TPS)
2. 3x Faster Query Performance
S3tables are purpose-build of tables, so as soon S3recognised theis dataset is in tabular format that you are storing in iceberg format. AWS specifically apply optimisation on our behalf 

**Performance benchmarks**
General purpose buckets: 
Scale by prefix
Adds TPS capacity under load, 
starts at 5500 reads/seconds
starts at 3500 writes/seconds
Under load S3 request additional resources to achieve the demand.

Table buckets
S3 storage optimised for iceberg key naming
TPS startes at 55000 read/seconds
35000 write/seconds
AWS contributed to Iceberg: in which you are writing your data and laying out your data files in Iceberg now is better optimized to be able to take advantage
of the automatic scaling that S3 provides. Blog post is also available [2]

**S3**
Originally build: 2006
Idea: Store the data securely & durably have it available when needed.
Envisioned filed stored: Video, Photos and other static content
Current usage: Data used for Analytics, Machine Learning and AI 
Usage: Meterological data, financial transactions and records
Format: Apache Parquet (Tabular format)

Problem with Parquet datafiles: Evolving the schema was difficult because adding and removing columns means that efforts is spent in making sure data can be consumed at speeed at scale. Does this mean we need to update schema in each and every application?
Iceberg: Adds metadata file, metadata allows to structure the data, safe transactions, time-travel, SQL semantic, allows consistency of the data

Iceberg features
1. Time travel
2. Schema information
3. Compaction





## Reference: 
1. https://www.youtube.com/watch?v=1U7yX4HTLCI
2. https://aws.amazon.com/blogs/storage/how-amazon-ads-uses-iceberg-optimizations-to-accelerate-their-spark-workload-on-amazon-s3/
